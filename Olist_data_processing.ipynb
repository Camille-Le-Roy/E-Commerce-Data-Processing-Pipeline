{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preamble\n",
    "\n",
    "Here I build a data processing pipeline using the <a href=\"https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\" target=\"_blank\" style=\"text-decoration:underline;\">Olist' (Brazilian E-Commerce) dataset</a>.\n",
    "The dataset includes over 100,000 orders from 2016 to 2018, providing a comprehensive view of e-commerce transactions in Brazil.\n",
    "It is a relational dataset, including 9 CSV tables linked by IDs.\n",
    "\n",
    "The dataset includes several interconnected tables, which can be merged to create a rich dataset for analysis.\n",
    "\n"
   ],
   "id": "a544aabe89b02f98"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loading datasets",
   "id": "1c184a88a0fe9111"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "local_path = Path.cwd()\n",
    "\n",
    "# path to data folder\n",
    "data_folder = local_path / \"data\" / \"Olist ecommerce dataset\"\n",
    "\n",
    "# Create a function to load all datasets located in \"data_folder\"\n",
    "def load_all_csv_data(folder_path):\n",
    "    dataframes = {}\n",
    "    csv_files_list = list(data_folder.glob(\"*.csv\")) #  Use Path.glob() to search for all \".csv\" file in the folder\n",
    "    for file_path in csv_files_list:\n",
    "        file_name = file_path.name\n",
    "        print(f\"Loading {file_name}...\")\n",
    "        try:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            # low_memory=False is used for better type inference on large files\n",
    "            df = pd.read_csv(\n",
    "                file_path,\n",
    "                low_memory=False # low_memory=False is used for better type inference on large files\n",
    "            )\n",
    "            # Store the DataFrame using the file name (without the extension) as the key\n",
    "            key_name = file_name.replace('.csv', '')\n",
    "            dataframes[key_name] = df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_name}: {e}\")\n",
    "    return dataframes\n",
    "\n",
    "datasets = load_all_csv_data(data_folder) # Create a 'data dictionary' that will hold all DataFrames found in the folder.\n",
    "datasets.keys()\n",
    "\n",
    "print(\"\\n--- Loaded DataFrames ---\")\n",
    "# Print the keys and the shape of each loaded DataFrame to confirm\n",
    "for name, df in datasets.items():\n",
    "    print(f\"'{name}': {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "\n",
    "### let's create individual datasets and give them handy names:\n",
    "df_orders = datasets['olist_orders_dataset'].copy()\n",
    "df_order_items = datasets['olist_order_items_dataset'].copy()\n",
    "df_customers = datasets['olist_customers_dataset'].copy()\n",
    "df_geolocation = datasets['olist_geolocation_dataset'].copy()\n",
    "df_order_payments = datasets['olist_order_payments_dataset'].copy()\n",
    "df_order_reviews = datasets['olist_order_reviews_dataset'].copy()\n",
    "df_products = datasets['olist_products_dataset'].copy()\n",
    "df_sellers = datasets['olist_sellers_dataset'].copy()\n",
    "df_product_name_translation = datasets['product_category_name_translation'].copy()"
   ],
   "id": "64f75ef662cb1fa1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Relationships between tables\n",
    "\n",
    "| Table                                           | Key                                   | Links to                                  | Description                        |\n",
    "| ----------------------------------------------- | ------------------------------------- | ----------------------------------------- | ---------------------------------- |\n",
    "| `df_orders`                                     | `order_id`                            | customers, order_items, reviews, payments | Core transaction table             |\n",
    "| `df_order_items`                                | `order_id`, `product_id`, `seller_id` | products, sellers                         | Each row = one product in an order |\n",
    "| `df_products`                                   | `product_id`                          | order_items                               | Product attributes                 |\n",
    "| `df_customers`                                  | `customer_id`                         | orders                                    | Buyer demographics and location    |\n",
    "| `df_sellers`                                    | `seller_id`                           | order_items                               | Seller info                        |\n",
    "| `df_geolocation`                                | `zip_code_prefix`                     | customers/sellers                         | Lat-long per postal code           |\n",
    "| `df_order_reviews`                              | `order_id`                            | —                                         | Ratings and comments               |\n",
    "| `df_order_payments`                             | `order_id`                            | —                                         | Payment method(s)                  |"
   ],
   "id": "6d734b3d4e510bd1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Cleaning",
   "id": "6fb16f2afc50d5a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean ORDERS",
   "id": "b19a640a51300381"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# \"df_orders\" is the core transaction table\n",
    "# it is linked to customers, order_items, reviews and payments via \"order_id\"\n",
    "\n",
    "df_orders.dtypes # show all columns type\n",
    "df_orders.order_status.unique() # check the different levels in 'order_status'\n",
    "# column 4th to 8th are timestamps\n",
    "timestamp_cols = [\n",
    "    'order_purchase_timestamp',\n",
    "    'order_approved_at',\n",
    "    'order_delivered_carrier_date',\n",
    "    'order_delivered_customer_date',\n",
    "    'order_estimated_delivery_date'\n",
    "]\n",
    "## convert dates column to datetime (pd.to_datetime)\n",
    "for col in timestamp_cols: # Iterate through all timestamp columns\n",
    "    df_orders[col] = pd.to_datetime(df_orders[col], errors='coerce') # Convert the data in each selected column to a proper datetime format; replace unparseable strings with NaT (Not a Time/missing value)\n",
    "df_orders.dtypes # check if it works\n",
    "\n",
    "## remove canceled orders:\n",
    "df_orders = df_orders[df_orders['order_status'] != 'canceled']\n",
    "\n",
    "##### Build delivery delay features\n",
    "\n",
    "df_orders['delivery_delay'] = (df_orders['order_delivered_customer_date'] - df_orders['order_estimated_delivery_date']).dt.days\n",
    "# Calculate the difference between the actual customer delivery date and the estimated delivery date,\n",
    "# then convert the result to the total number of days (using \".dt.days\").\n",
    "df_orders.delivery_delay.head()\n",
    "# A positive value means the delivery was late; a negative value means it was early.\n",
    "\n",
    "# Calculate the total number of days elapsed between the moment the order was purchased and when it was delivered to the customer.\n",
    "df_orders['purchase_to_delivery_days'] = (df_orders['order_delivered_customer_date'] - df_orders['order_purchase_timestamp']).dt.days\n",
    "df_orders.purchase_to_delivery_days.head()\n",
    "\n",
    "## check for missing values per columns\n",
    "df_orders.isnull().sum()\n",
    "## check how many order are NOT delivered (probably generating the NAs)\n",
    "print(df_orders['order_status'].value_counts())"
   ],
   "id": "2bceedbc577d5ac3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean ORDER_ITEMS",
   "id": "9effb9b7db31e0a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# in \"df_order_items\", each row = one product in an order.\n",
    "# it is linked to df_orders, df_products and df_sellers\n",
    "\n",
    "print(df_order_items.dtypes) # check columns type\n",
    "df_order_items['shipping_limit_date'] = pd.to_datetime(df_order_items['shipping_limit_date'], errors='coerce') # convert to proper datetime\n",
    "\n",
    "# Calculate a new feature: the ratio of the shipping cost (freight_value) to the item's price.\n",
    "df_order_items['shipping_cost_ratio'] = df_order_items['freight_value'] / df_order_items['price']\n",
    "len(df_order_items[df_order_items.price == 0]) # check if some price are 0\n",
    "\n",
    "# Aggregate at order_id level (sum of all items in same order)\n",
    "df_order_items_agg = df_order_items.groupby('order_id').agg({\n",
    "    'price': 'sum',           # Calculate the total price for all items within each order.\n",
    "    'freight_value': 'sum',   # Calculate the total shipping cost for all items within each order.\n",
    "    'order_item_id': 'count'  # Count the number of distinct items associated with each order (which equals the number of rows grouped).\n",
    "}).rename(columns={'order_item_id': 'n_items'}).reset_index()\n",
    "# ---> df_order_items_agg has one row per order_id. This is necessary for joining with the main df_orders table\n",
    "# Rename the counted column to 'n_items' for clarity, and convert the 'order_item_id' from the index to a regular column (with \"reset_index()\")."
   ],
   "id": "991d682ca5ea59b5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean PAYMENTS",
   "id": "671139079da1fecd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# df_order_payments include info on payment value and method\n",
    "# it is linked to df_orders via order_id\n",
    "\n",
    "print(df_order_payments.dtypes) # check columns type\n",
    "\n",
    "# Aggregate at order_id level (is one order has multiple payments -> aggregate)\n",
    "    'payment_sequential': 'max',  # Get the maximum payment sequence number (indicating the total count of distinct payments made for the order).\n",
    "df_order_payments_agg = df_order_payments.groupby('order_id').agg({\n",
    "    'payment_value': 'sum',       # Calculate the total amount paid for each order.\n",
    "    'payment_type': lambda x: x.mode().iloc[0] if len(x.mode()) else np.nan\n",
    "    # Determine the MOST FREQUENT (mode) payment type for the order. If there's a tie or no payment, return the first mode found or NaN.\n",
    "}).reset_index()\n",
    "# Convert the 'order_id' back from the index (used for grouping) into a regular column.\n",
    "# ---> df_order_payments_agg has one row per order_id."
   ],
   "id": "8d40bfd8bec57182"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean REVIEWS",
   "id": "951146fc73515609"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# df_order_reviews includes ratings and comments\n",
    "# it is linked to 'df_orders' via 'order_id'\n",
    "\n",
    "print(df_order_reviews.dtypes) # check columns type\n",
    "\n",
    "# The 2 last column are time stamp, we convert them both to proper datetime\n",
    "df_order_reviews['review_creation_date'] = pd.to_datetime(df_order_reviews['review_creation_date'], errors='coerce')\n",
    "df_order_reviews['review_answer_timestamp'] = pd.to_datetime(df_order_reviews['review_answer_timestamp'], errors='coerce')\n",
    "\n",
    "# Aggregate rating are the order_id level\n",
    "df_order_reviews_agg = df_order_reviews.groupby('order_id')['review_score'].mean().rename('avg_review').reset_index()\n",
    "# Group the reviews by unique order_id and calculate the average review_score for each order.\n",
    "# Rename the resulting Series to 'avg_review' and convert 'order_id' back from the index to a column for merging."
   ],
   "id": "52522a360019e6dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean CUSTOMERS",
   "id": "1df0272b18bb888d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# df_customers includes buyer demographics and location\n",
    "# It is linked to 'df_orders' via 'customer_id'\n",
    "\n",
    "print(df_customers.dtypes) # check columns type\n",
    "\n",
    "## is there any duplicate 'customer_unique_id'?\n",
    "print(f\"There is {int(df_customers.duplicated(subset='customer_id', keep=False).sum())} duplicates in 'customer_id'\")"
   ],
   "id": "de644990a90c2339"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean SELLERS",
   "id": "b8bcec21b9d04865"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# df_sellers includes seller info\n",
    "# It is linked to 'order_items' via 'seller_id'\n",
    "\n",
    "print(df_sellers.dtypes) # check columns type\n",
    "\n",
    "## is there any duplicate 'seller_id'?\n",
    "print(f\"There is {int(df_sellers.duplicated(subset='seller_id', keep=False).sum())} duplicates in 'df_sellers'\")"
   ],
   "id": "ee9f45a71d36b8ae"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean PRODUCTS",
   "id": "d7972d6aadae6805"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# df_products includes product attributes\n",
    "# It is linked to 'df_order_items' via 'product_id'\n",
    "\n",
    "print(df_products.dtypes) # check columns type\n",
    "\n",
    "# for now, we do nothing with this table"
   ],
   "id": "75e57bba7e7cb55b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Clean GEOLOCATION",
   "id": "24eb6429bf3cfa75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# df_geolocation includes latitude and longitude per postal code\n",
    "# It is linked to 'df_customers' and 'df_sellers' via zip code prefix\n",
    "\n",
    "print(df_geolocation.dtypes) # check columns type\n",
    "\n",
    "# for now, we do nothing with this table"
   ],
   "id": "92091d4d6ed41707"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Summary of clean tables",
   "id": "45c138ab00e62a0b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(f\"orders: {df_orders.shape}\")\n",
    "print(f\"order_items_agg: {df_order_items_agg.shape}\")\n",
    "print(f\"payment_agg: {df_order_payments_agg.shape}\")\n",
    "print(f\"review_agg: {df_order_reviews_agg.shape}\")\n",
    "print(f\"customers: {df_customers.shape}\")\n",
    "print(f\"products: {df_products.shape}\")\n",
    "print(f\"sellers: {df_sellers.shape}\")\n",
    "print(f\"geo: {df_geolocation.shape}\")"
   ],
   "id": "f8499aa09b172c46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Merge Datasets",
   "id": "635248f6b97b498d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "GOAL: Build an Analytical Base Table (ABT) by combining all cleaned datasets\n",
    "We create two main tables:\n",
    "1. orders_merged: ONE ROW PER ORDER (for order-level analysis)\n",
    "2. df_order_items: ONE ROW PER PRODUCT (for item-level analysis)\n"
   ],
   "id": "b259348ad883d46c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Merge Order-Level Data",
   "id": "beff484d116d4aca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Create a comprehensive order table where each row represents ONE complete order\n",
    "We start with df_orders as our base and progressively add information from other tables"
   ],
   "id": "6b195fb1fc0fe2fe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "orders_merged = (\n",
    "    df_orders  # BASE TABLE: Start with the core orders table (defines which rows we keep)\n",
    "\n",
    "    # Add aggregated item information (total price, freight, number of items per order)\n",
    "    .merge(df_order_items_agg, on='order_id', how='left')\n",
    "    # Brings in: 'price' (sum), 'freight_value' (sum), 'n_items' (count\n",
    "\n",
    "    # Add aggregated payment information (total payment value, payment type per order)\n",
    "    .merge(df_order_payments_agg, on='order_id', how='left')\n",
    "    # Brings in: 'payment_value' (sum), 'payment_sequential' (max), 'payment_type' (mode)\n",
    "\n",
    "    # Add aggregated review information (average review score per order)\n",
    "    .merge(df_order_reviews_agg, on='order_id', how='left')\n",
    "    # Brings in: 'avg_review' (mean of all review scores for this order)\n",
    "\n",
    "    # Add customer information (demographics and location)\n",
    "    .merge(df_customers,\n",
    "           left_on='customer_id',  # Column in 'df_orders'\n",
    "           right_on='customer_id',  # match in 'df_customers'\n",
    "           how='left')\n",
    "    # Brings in: 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state'\n",
    ")\n",
    "\n",
    "# RESULT: orders_merged now has ONE ROW per order_id with complete order information\n",
    "print(orders_merged.dtypes)\n",
    "\n",
    "print(f\"The 'orders_merged' table includes {orders_merged.shape[0]} individual orders, one per row.\\n\"\n",
    "      f\"Note that {df_customers.customer_unique_id.duplicated().sum()} orders were placed by customers who had ordered previously,\\n\"\n",
    "      f\" i.e. there are {df_customers.customer_unique_id.duplicated().sum()} duplicates in the column customer_unique_id.\")\n",
    "\n"
   ],
   "id": "52a2d6ea87e5eb30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note: One customer can place multiple orders, so:\n",
    "- Number of rows = number of transactions/orders\n",
    "- Number of unique customer_id = number of unique people who made purchases\n",
    "\n",
    "The column 'customer_unique_id' in the raw customer table includes 3345 replicates\n",
    "This is because 'customer_unique_id' identifies the person (the individual), and the duplicates represent the total number of repeat orders placed by existing customers."
   ],
   "id": "8bdaf4b94b6e240c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Enrich Item-Level Data (Product + Seller Info)",
   "id": "dab5cb4cba1da200"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Now we work on df_order_items to add product and seller details\n",
    "# Each row here represents ONE PRODUCT within an order (not aggregated)\n",
    "\n",
    "# STEP 1: Add product information (category name in Portuguese, weight)\n",
    "order_items_merged = df_order_items.merge(\n",
    "    df_products[['product_id', 'product_category_name', 'product_weight_g']],  # Select only needed columns\n",
    "    on='product_id',  # Match by product_id\n",
    "    how='left'  # Keep all items, even if product info is missing (fills with NaN)\n",
    ")\n",
    "# Brings in: 'product_category_name' (Portuguese), 'product_weight_g'\n",
    "\n",
    "# STEP 2: Add seller information (location details)\n",
    "order_items_merged = order_items_merged.merge(\n",
    "    df_sellers,  # Contains: seller_id, seller_zip_code_prefix, seller_city, seller_state\n",
    "    on='seller_id',  # Match by seller_id\n",
    "    how='left'  # Keep all items, even if seller info is missing\n",
    ")\n",
    "# Brings in: 'seller_zip_code_prefix', 'seller_city', 'seller_state'\n",
    "\n",
    "# STEP 3: Translate product category names from Portuguese to English\n",
    "order_items_merged = order_items_merged.merge(\n",
    "    df_product_name_translation,  # Lookup table: product_category_name (PT) → product_category_name_english (EN)\n",
    "    left_on='product_category_name',  # Column in order_items_merged (Portuguese name)\n",
    "    right_on='product_category_name',  # Column in translation table (Portuguese name)\n",
    "    how='left'  # Keep all items, even if translation is missing\n",
    ")\n",
    "# NOTE: We use 'left_on' and 'right_on' here because both tables have the same column name\n",
    "# After merge, we get: 'product_category_name_english' (the translated English name)\n",
    "\n",
    "# STEP 4: Clean up duplicate column created during translation merge\n",
    "order_items_merged.drop(columns=['product_category_name'], inplace=True)\n",
    "# This removes the 'product_category_name' because we just want to keep 'product_category_name_english'\n",
    "\n",
    "print(f\"Item-level table shape: {order_items_merged.shape}\")\n",
    "# RESULT: order_items_merged now has ONE ROW per product with complete product + seller info"
   ],
   "id": "f089db4790700864"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Add Geolocation Data (Seller & Customer Coordinates)",
   "id": "7d29a1717f9d00e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "GOAL: Calculate the physical distance between seller and customer for each order\n",
    "This requires getting latitude/longitude for both sellers and customers"
   ],
   "id": "226d3147f2800281"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# STEP 1: Get seller coordinates by merging sellers with geolocation data\n",
    "df_sellers_geo = df_sellers.merge(\n",
    "    df_geolocation[['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng']],  # Select only the key + coordinate columns\n",
    "    left_on='seller_zip_code_prefix',  # Seller's ZIP code (in df_sellers)\n",
    "    right_on='geolocation_zip_code_prefix',  # Matching ZIP code (in df_geolocation)\n",
    "    how='left'  # Keep all sellers, even if coordinates are missing\n",
    ")\n",
    "\n",
    "df_sellers_geo['seller_id'].duplicated().sum() # seller_id has duplicate!\n",
    "# -->We keep only the first lat/lng per seller\n",
    "df_sellers_geo_unique = df_sellers_geo.drop_duplicates(subset='seller_id')\n",
    "\n",
    "# Rename columns for clarity (specify we're talking about seller coordinates)\n",
    "df_sellers_geo_unique = df_sellers_geo_unique.rename(columns={\n",
    "    'geolocation_lat': 'seller_lat',\n",
    "    'geolocation_lng': 'seller_lng'\n",
    "}, inplace=False)\n",
    "# RESULT: df_sellers_geo has latitude/longitude for each seller\n",
    "\n",
    "# STEP 2: Add seller coordinates to the item-level table\n",
    "order_items_merged = order_items_merged.merge(\n",
    "    df_sellers_geo_unique[['seller_id', 'seller_lat', 'seller_lng']],  # Select only the key (the ID) and coordinates\n",
    "    on='seller_id',  # Match by seller_id\n",
    "    how='left'  # Keep all items, even if seller coordinates are missing\n",
    ")\n",
    "# RESULT: Each product in order_items_merged now has its seller's coordinates\n",
    "\n",
    "# STEP 3: Get customer coordinates by merging customers with geolocation data\n",
    "df_customers_geo = df_customers.merge(\n",
    "    df_geolocation[['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng']],  # Select the key and the coordinate columns\n",
    "    left_on='customer_zip_code_prefix',  # Customer's ZIP code (in df_customers)\n",
    "    right_on='geolocation_zip_code_prefix',  # Matching ZIP code (in df_geolocation)\n",
    "    how='left'  # Keep all customers, even if coordinates are missing\n",
    ").rename(columns={\n",
    "    'geolocation_lat': 'customer_lat',\n",
    "    'geolocation_lng': 'customer_lng'\n",
    "})\n",
    "# RESULT: df_customers_geo has latitude/longitude for each customer\n",
    "\n",
    "# STEP 4: Add customer coordinates to orders\n",
    "df_orders_geo = df_orders.merge(\n",
    "    df_customers_geo[['customer_id', 'customer_lat', 'customer_lng']],  # Select only ID and coordinates\n",
    "    on='customer_id',  # Match by customer_id\n",
    "    how='left'  # Keep all orders, even if customer coordinates are missing\n",
    ")\n",
    "# RESULT: Each order in df_orders_geo now has its customer's coordinates"
   ],
   "id": "14dd0643e9018318"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Calculate Distance Between Seller and Customer",
   "id": "46cf651068a9c676"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "CHALLENGE: One order can have multiple sellers (if buying from different sellers)\n",
    "SOLUTION: For simplicity, we take the FIRST seller per order for distance calculation"
   ],
   "id": "a0b144cd442f9a1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# STEP 1: Get one seller's coordinates per order (takes the first seller if multiple exist)\n",
    "seller_coords_per_order = order_items_merged.groupby('order_id')[['seller_lat', 'seller_lng']].first().reset_index()\n",
    "# This creates a table with ONE ROW per order_id containing one seller's coordinates\n",
    "\n",
    "# STEP 2: Combine seller and customer coordinates in one table\n",
    "orders_distance = df_orders_geo[['order_id', 'customer_lat', 'customer_lng']].merge(\n",
    "    seller_coords_per_order,  # Contains: order_id, seller_lat, seller_lng\n",
    "    on='order_id',  # Match by order_id\n",
    "    how='left'  # Keep all orders, even if seller coordinates are missing\n",
    ")\n",
    "# RESULT: One row per order with both seller and customer coordinates\n",
    "\n",
    "# STEP 3: Calculate geodesic distance (shortest distance on Earth's surface)\n",
    "from geopy.distance import geodesic\n",
    "from tqdm.auto import tqdm # Import tqdm for showing a progress bar\n",
    "tqdm.pandas() # Activate tqdm's integration with Pandas\n",
    "\n",
    "def calc_distance_vec(row):\n",
    "    \"\"\"\n",
    "    Calculate distance in kilometers between seller and customer.\n",
    "    Uses geodesic distance (accounts for Earth's curvature).\n",
    "    Returns NaN if coordinates are missing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create coordinate tuples: (latitude, longitude)\n",
    "        seller_location = (row['seller_lat'], row['seller_lng'])\n",
    "        customer_location = (row['customer_lat'], row['customer_lng'])\n",
    "        # Calculate distance using geopy's geodesic function\n",
    "        return geodesic(seller_location, customer_location).km\n",
    "    except:\n",
    "        # Return NaN if calculation fails (missing coordinates, invalid values, etc.)\n",
    "        return np.nan\n",
    "\n",
    "# Apply the distance calculation (and display a progress bar with 'progress_apply')\n",
    "# orders_distance['distance_km'] = orders_distance.progress_apply(calc_distance_vec, axis=1) # !!! VERY LONG TO RUN !!!\n",
    "# axis=1 means apply function to each ROW (not each column)\n",
    "\n",
    "## save the distance calculation because it's long to run:\n",
    "# dist = orders_distance['distance_km']\n",
    "output_folder = local_path / \"data\"/ \"Olist ecommerce dataset\" / \"processed_datasets\"\n",
    "# dist.to_csv(output_folder / \"distance_km.csv\", index=False)\n",
    "orders_distance['distance_km'] = pd.read_csv(output_folder / \"distance_km.csv\", low_memory=False)\n",
    "\n",
    "## Ensure that there is one row per order_id in orders_distance:\n",
    "orders_distance_unique = orders_distance.groupby('order_id')[['customer_lat', 'customer_lng', 'seller_lat', 'seller_lng', 'distance_km']].first().reset_index()\n",
    "\n",
    "# STEP 4: Merge the calculated distance back into the main order table\n",
    "orders_merged = orders_merged.merge(\n",
    "    orders_distance_unique[['order_id', 'distance_km']],  # Select only order_id and the distance\n",
    "    on='order_id',  # Match by order_id\n",
    "    how='left'  # Keep all orders, even if distance is missing\n",
    ")\n",
    "\n",
    "# FINAL RESULT: orders_merged now has distance_km for logistics analysis\n",
    "print(f\"Final orders_merged shape: {orders_merged.shape}\")"
   ],
   "id": "c005a2c357726244"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Summary of merged tables",
   "id": "63db7965acfbf104"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "TWO MAIN TABLES CREATED:\n",
    "\n",
    "1. orders_merged (ORDER-LEVEL):\n",
    "   - One row per order\n",
    "   - Contains: order details, aggregated items/payments/reviews, customer info, distance\n",
    "   - Use for: \"What's the average delivery time?\", \"Which states order most?\", etc.\n",
    "\n",
    "2. order_items_merged (ITEM-LEVEL):\n",
    "   - One row per product within each order\n",
    "   - Contains: item details, product info, seller info, seller coordinates\n",
    "   - Use for: \"Which categories sell best?\", \"Which sellers are most popular?\", etc."
   ],
   "id": "208293efa1d576cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Feature Engineering",
   "id": "21ab7f342a865851"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GOAL: compute new features derived from the existing ones to enable richer analysis",
   "id": "164723aae384733c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Order-Level Analysis\n",
    "---> working on 'orders_merged' dataset"
   ],
   "id": "1aece40d3f939b04"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Time-Based Features",
   "id": "9fa67252fae06481"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Extract temporal components for seasonality analysis\n",
    "orders_merged['order_year'] = orders_merged['order_purchase_timestamp'].dt.year\n",
    "orders_merged['order_month'] = orders_merged['order_purchase_timestamp'].dt.month\n",
    "orders_merged['order_day_of_week'] = orders_merged['order_purchase_timestamp'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "orders_merged['order_hour'] = orders_merged['order_purchase_timestamp'].dt.hour\n",
    "orders_merged['is_weekend'] = orders_merged['order_day_of_week'].isin([5, 6]).astype(int)  # 1 if Sat/Sun\n",
    "orders_merged['is_holiday_season'] = orders_merged['order_month'].isin([11, 12]).astype(int)  # Nov-Dec\n",
    "\n",
    "# Calculates the time between the purchase being made and the payment being officially approved.\n",
    "# (= payment processing speed).\n",
    "orders_merged['approval_delay_hours'] = (\n",
    "    orders_merged['order_approved_at'] - orders_merged['order_purchase_timestamp']\n",
    ").dt.total_seconds() / 3600\n",
    "\n",
    "# Calculate the time elapsed between the order being approved and the package being picked up by the shipping carrier.\n",
    "# (= seller's internal handling speed).\n",
    "orders_merged['carrier_pickup_days'] = (\n",
    "    orders_merged['order_delivered_carrier_date'] - orders_merged['order_approved_at']\n",
    ").dt.days"
   ],
   "id": "4e0e1c5c10d5006f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Order Value & Economics",
   "id": "26775e4ca9b025e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Average price per item\n",
    "orders_merged['avg_price_per_item'] = orders_merged['price'] / orders_merged['n_items']\n",
    "\n",
    "# Shipping cost as percentage of order value\n",
    "orders_merged['freight_percentage'] = (orders_merged['freight_value'] / orders_merged['price']) * 100\n",
    "\n",
    "# Total order value (price + freight)\n",
    "orders_merged['total_order_value'] = orders_merged['price'] + orders_merged['freight_value']\n",
    "\n",
    "# Revenue per km (efficiency metric)\n",
    "orders_merged['revenue_per_km'] = orders_merged['price'] / orders_merged['distance_km']\n",
    "\n",
    "# High-value order flag (e.g., top 10% of orders)\n",
    "orders_merged['is_high_value'] = (\n",
    "    orders_merged['price'] > orders_merged['price'].quantile(0.9)\n",
    ").astype(int)"
   ],
   "id": "5c548df33380ea8c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Delivery Performance",
   "id": "520bf2a296535fed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create delivery performance categories: 'Early', 'On Time', or 'Late'\n",
    "orders_merged['delivery_status'] = orders_merged['delivery_delay'].apply(\n",
    "    lambda x: 'Early' if x < -2 # Assign 'Early' if the delivery delay is less than -2 days (meaning it arrived 3 or more days before the estimated date).\n",
    "    else ('On Time' if x <= 2   # Assign 'On Time' if the delivery delay is between -2 days (inclusive) and +2 days (inclusive).\n",
    "          else 'Late')          # Otherwise (if the delay is greater than 2 days), assign 'Late'.\n",
    ")\n",
    "\n",
    "# Late delivery flag\n",
    "orders_merged['is_late'] = (orders_merged['delivery_delay'] > 0).astype(int)\n",
    "\n",
    "# Delivery speed category (SUBJECTIVE CATEGORIZATION ?)\n",
    "orders_merged['delivery_speed'] = pd.cut( # pd.cut() is used to segment continuous data into discrete intervals\n",
    "    orders_merged['purchase_to_delivery_days'], # The numerical column representing the total days from purchase to customer delivery.\n",
    "    bins=[0, 7, 14, 30, 999], # Define the boundaries (bins) for the categories, measured in days.\n",
    "    labels=['Express', 'Fast', 'Standard', 'Slow'] # Assign a descriptive label to each resulting bin/interval.\n",
    ")"
   ],
   "id": "48dd2cc7126cbeb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Customer Satisfaction",
   "id": "62ddccb5a62b0ba9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Binary satisfaction (good vs bad reviews)\n",
    "orders_merged['is_satisfied'] = (orders_merged['avg_review'] >= 4).astype(int)\n",
    "\n",
    "# Review category\n",
    "orders_merged['review_category'] = pd.cut(\n",
    "    orders_merged['avg_review'],\n",
    "    bins=[0, 2, 3, 4, 5],\n",
    "    labels=['Poor', 'Fair', 'Good', 'Excellent']\n",
    ")"
   ],
   "id": "ec8db63f12c470a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Logistics Complexity",
   "id": "1f8e4ee44ef5867b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## Distance categories for logistics planning\n",
    "orders_merged['distance_category'] = pd.cut(\n",
    "    orders_merged['distance_km'],\n",
    "    bins=[0, 100, 500, 1000, 5000],\n",
    "    labels=['Local', 'Regional', 'National', 'Long Distance']\n",
    ")\n",
    "\n",
    "## Complexity score (combining items + distance)\n",
    "orders_merged['logistics_complexity'] = (\n",
    "    orders_merged['n_items'] * orders_merged['distance_km'] / 100\n",
    ")\n",
    "# This calculates a composite score by multiplying the number of items by the delivery distance (in km) and scaling the result down by dividing by 100 (preventing the nb to become extremely large).\n",
    "# This score represents the combined effort or cost required to fulfill the order."
   ],
   "id": "75525ebf79a545af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Item-Level Analysis\n",
    "---> working on 'order_items_merged' dataset"
   ],
   "id": "894ee6a15406fbe7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Product Economics",
   "id": "f81a11ac2de5ae63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Product contribution to order\n",
    "# --> Calculate the proportion that the price of an individual item contributes to the total price of its respective order.\n",
    "order_items_merged['item_share_of_order'] = order_items_merged.groupby('order_id')['price'].transform( # Group by 'order_id' and focus on 'price'\n",
    "    lambda x: x / x.sum() # divides the price of each individual item (x) by the order's total price (x.sum())\n",
    ") # Result: The 'transform' method broadcasts the result back to the original DataFrame, ensuring the new feature has the same length as the original 'order_items_merged' table.\n",
    "\n",
    "# Weight-to-price ratio (shipping efficiency)\n",
    "order_items_merged['weight_per_dollar'] = (\n",
    "    order_items_merged['product_weight_g'] / order_items_merged['price']\n",
    ")\n",
    "\n",
    "# Freight efficiency per gram\n",
    "order_items_merged['freight_per_gram'] = (\n",
    "    order_items_merged['freight_value'] / order_items_merged['product_weight_g']\n",
    ")"
   ],
   "id": "30f5000c0bacea8d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Product Categories",
   "id": "29c9725949bdc4b2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Heavy vs light products\n",
    "order_items_merged['is_heavy_product'] = (\n",
    "    order_items_merged['product_weight_g'] > order_items_merged['product_weight_g'].median()\n",
    ").astype(int)\n",
    "\n",
    "# Premium product flag (top 25% by price)\n",
    "order_items_merged['is_premium'] = (\n",
    "    order_items_merged['price'] > order_items_merged['price'].quantile(0.75)\n",
    ").astype(int)"
   ],
   "id": "11deca7477b35d58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Seller Performance",
   "id": "35df0382e8464f53"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create seller-level aggregations, then merge back\n",
    "seller_stats = order_items_merged.groupby('seller_id').agg({\n",
    "    'order_id': 'nunique',  # Number of unique orders\n",
    "    'price': 'mean'  # Average item price\n",
    "}).rename(columns={'order_id': 'seller_order_count', 'price': 'seller_avg_price'}) # Rename the resulting aggregated columns to be descriptive.\n",
    "\n",
    "order_items_merged = order_items_merged.merge(seller_stats, on='seller_id', how='left')\n",
    "# Merge the calculated 'seller_stats' DataFrame back into the 'order_items_merged' DataFrame.\n",
    "# Each item row now inherits the seller's total order count and average price.\n",
    "\n",
    "# Top seller flag (e.g., sellers with 100+ orders)\n",
    "order_items_merged['is_top_seller'] = (\n",
    "    order_items_merged['seller_order_count'] >= 100\n",
    ").astype(int)"
   ],
   "id": "c83a1765aa8c4def"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Geographic Features",
   "id": "f91476f616e1edc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Same state transaction (seller and customer in same state)\n",
    "# First need to merge customer_state into order_items_merged\n",
    "order_items_merged = order_items_merged.merge(\n",
    "    orders_merged[['order_id', 'customer_state']],\n",
    "    on='order_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "order_items_merged['is_local_transaction'] = (\n",
    "    order_items_merged['seller_state'] == order_items_merged['customer_state']\n",
    ").astype(int)"
   ],
   "id": "851082a326d81ead"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Customer Behavior",
   "id": "c3fd3a2c4fde9cb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate at the customer-level (merge back to orders_merged)\n",
    "customer_features = orders_merged.groupby('customer_unique_id').agg({\n",
    "    'order_id': 'count',  # Frequency\n",
    "    'order_purchase_timestamp': 'max',  # Recency\n",
    "    'price': 'sum'  # Monetary\n",
    "}).rename(columns={\n",
    "    'order_id': 'customer_order_count',\n",
    "    'order_purchase_timestamp': 'customer_last_order',\n",
    "    'price': 'customer_lifetime_value' # it's actually the Historical Customer Value (HCV) on the dataset period, but a good proxy of CLV\n",
    "})\n",
    "\n",
    "# Days since last order (as of dataset end date)\n",
    "dataset_end_date = orders_merged['order_purchase_timestamp'].max()\n",
    "customer_features['days_since_last_order'] = (\n",
    "    dataset_end_date - customer_features['customer_last_order']\n",
    ").dt.days\n",
    "\n",
    "# Merge back in 'orders_merged'\n",
    "orders_merged = orders_merged.merge(customer_features, on='customer_unique_id', how='left')\n",
    "# Merges the three RFM features (count, last order date, and CLV) and the 'days_since_last_order' feature back into the main order table.\n",
    "# *** ---> Note that, for instance, if a customer placed 5 orders,\n",
    "#          there will be 5 rows in orders_merged containing the exact same customer_order_count and customer_lifetime_value.\n",
    "\n",
    "# Customer segment\n",
    "orders_merged['is_repeat_customer'] = (orders_merged['customer_order_count'] > 1).astype(int)\n",
    "# Creates a binary flag (1 or 0) indicating whether the order was placed by a customer who has made more than one order in total.\n",
    "# This feature is essential for analyzing loyalty and retention."
   ],
   "id": "4ab22c680e5c1d5c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Product Popularity",
   "id": "1834093da9ac1021"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Product-level aggregations\n",
    "product_stats = order_items_merged.groupby('product_category_name_english').agg({\n",
    "    'order_id': 'nunique', # Calculates the total number of unique orders placed for each product category (Frequency).\n",
    "    'price': 'mean'  # Calculates the average price of all items sold within each product category (Monetary).\n",
    "}).rename(columns={\n",
    "    'order_id': 'category_order_count', # Renames the unique order count to a descriptive frequency metric.\n",
    "    'price': 'category_avg_price' # Renames the mean price to the category's average value.\n",
    "})\n",
    "\n",
    "# Merge back in the item-level table\n",
    "order_items_merged = order_items_merged.merge(\n",
    "    product_stats,\n",
    "    on='product_category_name_english', # Joins the summary table back to the item-level table using the category name as the key.\n",
    "    how='left' # Ensures all item records remain in the table, with each item inheriting its category's statistics.\n",
    ")\n",
    "\n",
    "# Popular category flag (top 20% by order count)\n",
    "order_items_merged['is_popular_category'] = (\n",
    "    order_items_merged['category_order_count'] >\n",
    "    order_items_merged['category_order_count'].quantile(0.8) # Calculates the value of the 80th percentile for the 'category_order_count' feature.\n",
    ").astype(int)\n",
    "# Creates a binary flag (1 or 0) indicating whether the category belongs to the top 20% of categories based on the total number of orders received.\n",
    "# This discretizes the frequency metric into a clear 'popular' vs. 'not popular' feature for modeling."
   ],
   "id": "7fdf42936a7ee9a4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Summary of engineered features\n",
    "\n",
    "| Feature Type         | Business Use Case                                 |\n",
    "|----------------------|---------------------------------------------------|\n",
    "| Time-based           | Identify peak ordering times, seasonal trends     |\n",
    "| Order economics      | Optimize pricing, understand shipping costs       |\n",
    "| Delivery performance | Improve logistics, reduce late deliveries         |\n",
    "| Customer satisfaction| Predict churn, improve service quality            |\n",
    "| Seller performance   | Identify top sellers, optimize seller mix         |\n",
    "| Geographic           | Regional marketing, warehouse placement           |\n",
    "| Customer behavior    | Personalization, retention campaigns              |\n",
    "\n",
    "These features will enable richer analysis like:\n",
    "- Predicting delivery delays\n",
    "- Identifying factors affecting customer satisfaction\n",
    "- Segmenting customers/products for targeted strategies\n",
    "- Optimizing logistics based on distance/weight/value"
   ],
   "id": "d31abff2ee22c226"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(orders_merged.shape)\n",
    "print(orders_merged.isnull().sum())\n",
    "\n",
    "print(order_items_merged.shape)\n",
    "print(order_items_merged.isnull().sum())"
   ],
   "id": "64a26249dd07e6ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Create Customer-level Dataset\n",
    "\n",
    "The 'orders_merged' table is at the order levels. We built a customer-level dataset derived from the 'orders_merged' table:"
   ],
   "id": "9b565fa32f6ea91f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "### Aggregate to customer level\n",
    "\n",
    "# Define end date for recency calculations\n",
    "dataset_end_date = orders_merged['order_purchase_timestamp'].max()\n",
    "\n",
    "df_customers_agg = orders_merged.groupby('customer_unique_id').agg(\n",
    "    # --- Activity ---\n",
    "    n_orders=('order_id', 'nunique'),\n",
    "    first_order_date=('order_purchase_timestamp', 'min'),\n",
    "    last_order_date=('order_purchase_timestamp', 'max'),\n",
    "\n",
    "    # --- Monetary metrics ---\n",
    "    total_revenue=('total_order_value', 'sum'),\n",
    "    avg_order_value=('total_order_value', 'mean'),\n",
    "    median_order_value=('total_order_value', 'median'),\n",
    "    total_freight_paid=('freight_value', 'sum'),\n",
    "    avg_freight_percentage=('freight_percentage', 'mean'),\n",
    "\n",
    "    # --- Time / frequency metrics ---\n",
    "    avg_days_between_orders=('days_since_last_order', 'mean'),\n",
    "\n",
    "    # --- Satisfaction / experience ---\n",
    "    avg_review=('avg_review', 'mean'),\n",
    "    satisfaction_rate=('is_satisfied', 'mean'),\n",
    "\n",
    "    # --- Delivery / logistics ---\n",
    "    avg_delivery_delay=('delivery_delay', 'mean'),\n",
    "    late_order_rate=('is_late', 'mean'),\n",
    "\n",
    "    # --- Distance & geography ---\n",
    "    avg_distance_km=('distance_km', 'mean'),\n",
    "    most_frequent_state=('customer_state', lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan),\n",
    "    most_frequent_city=('customer_city', lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan),\n",
    "\n",
    "    # --- Order value distribution ---\n",
    "    pct_high_value_orders=('is_high_value', 'mean'),\n",
    "\n",
    "    # --- Review category (optional) ---\n",
    "    most_common_review_category=('review_category', lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "### Add recency and lifetime metrics for each customer:\n",
    "\n",
    "df_customers_agg['days_since_last_order'] = (dataset_end_date - df_customers_agg['last_order_date']).dt.days\n",
    "# Recency: Calculates the number of days between the customer's most recent order and the final date in the dataset.\n",
    "# A lower value indicates a more active customer.\n",
    "\n",
    "df_customers_agg['customer_lifetime_days'] = (df_customers_agg['last_order_date'] - df_customers_agg['first_order_date']).dt.days\n",
    "# Tenure/Longevity: Calculates the total duration (in days) between the customer's first and last order.\n",
    "# # A higher value indicates a longer relationship with the platform.\n",
    "\n",
    "# Loyalty / retention flags\n",
    "df_customers_agg['is_repeat_customer'] = (df_customers_agg['n_orders'] > 1).astype(int)\n",
    "# Creates a binary flag (1 or 0) indicating whether the customer has placed more than one order in total.\n",
    "# This is a key feature for loyalty and retention studies.\n",
    "\n",
    "# Average revenue per km (if distance exists)\n",
    "df_customers_agg['revenue_per_km'] = df_customers_agg['total_revenue'] / df_customers_agg['avg_distance_km']\n",
    "# Calculates how efficiently revenue is generated relative to the average shipping distance for that customer.\n",
    "# This is useful for logistics and profitability analysis; customers with high revenue_per_km are logistically less costly to serve relative to their spend.\n",
    "\n",
    "# RFM-like features\n",
    "# df_customers_agg['recency'] = df_customers_agg['days_since_last_order']\n",
    "# df_customers_agg['frequency'] = df_customers_agg['n_orders']\n",
    "# df_customers_agg['monetary'] = df_customers_agg['total_revenue']\n",
    "\n",
    "\n",
    "print(df_customers_agg.shape)"
   ],
   "id": "66955030e0d72ce5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save processed datasets",
   "id": "d9c9301802ae01df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create an output folder if it doesn't exist\n",
    "output_folder = local_path / \"data\"/ \"Olist ecommerce dataset\" / \"processed_datasets\"\n",
    "output_folder.mkdir(parents=True, exist_ok=True) # Create the directory specified by 'output_folder'\n",
    "\n",
    "### Save as CSV (human-readable, universally compatible)\n",
    "# orders_merged.to_csv(output_folder / \"orders_merged.csv\", index=False)\n",
    "# order_items_merged.to_csv(output_folder / \"order_items_merged.csv\", index=False)\n",
    "# df_customers_agg.to_csv(output_folder / \"customers_level_dataset.csv\", index=False)\n",
    "# print(f\"✓ Datasets saved to: {output_folder}\")\n",
    "# print(f\"  - orders_merged.csv: {orders_merged.shape}\")\n",
    "# print(f\"  - order_items_merged.csv: {order_items_merged.shape}\")\n",
    "# print(f\"  - customers_level_dataset.csv: {df_customers_agg.shape}\")\n",
    "\n",
    "### Save as Pickle (for Python-only workflows)\n",
    "# orders_merged.to_pickle(output_folder / \"orders_merged.pkl\")\n",
    "# order_items_merged.to_pickle(output_folder / \"order_items_merged.pkl\")"
   ],
   "id": "362c8bab309d009a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
